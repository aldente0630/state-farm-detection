{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3729c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_hub as hub\n",
    "from albumentations import (\n",
    "    Compose, \n",
    "    ShiftScaleRotate,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    ShuffleSplit,\n",
    ")\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils.common import (\n",
    "    get_cpu_count,\n",
    "    get_elapsed_time,\n",
    ")\n",
    "from utils.data_utils import (\n",
    "    load_tfrecord_dataset,\n",
    "    mixup_dataset,\n",
    ")\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_image(dataset, n_samples, label_names=None):\n",
    "    images, labels = next(iter(dataset))\n",
    "    images = images.numpy()\n",
    "    labels = labels.numpy()\n",
    "\n",
    "    n_cols = 5\n",
    "    n_rows = n_samples // n_cols if n_samples % n_cols == 0 else n_samples // n_cols + 1\n",
    "    fig = plt.figure(figsize=(n_cols * 4, n_rows * 3))\n",
    "    for i in range(n_samples):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(images[i])\n",
    "        label = (\n",
    "            labels[i].decode(\"utf-8\")\n",
    "            if label_names is None\n",
    "            else label_names[\"c\" + str(np.argmax(labels[i]))]\n",
    "        )\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "        ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "_ = config.read(os.path.join(\"..\", \"conf\", \"config.ini\"))\n",
    "\n",
    "project_name = config[\"project\"][\"project_name\"]\n",
    "run_name = config[\"project\"][\"run_name\"]\n",
    "raw_data_path = config[\"project\"][\"raw_data_path\"]\n",
    "validate_by_driver = eval(config[\"project\"][\"validate_by_driver\"])\n",
    "n_tfrec_chunks = eval(config[\"project\"][\"n_tfrec_chunks\"])\n",
    "\n",
    "model_url = config[\"model\"][\"model_url\"]\n",
    "fc_size = eval(config[\"model\"][\"fc_size\"])\n",
    "img_size = eval(config[\"model\"][\"img_size\"])\n",
    "n_epochs = eval(config[\"model\"][\"n_epochs\"])\n",
    "batch_size = eval(config[\"model\"][\"batch_size\"])\n",
    "initial_learning_rate = eval(config[\"model\"][\"initial_learning_rate\"])\n",
    "first_decay_steps = eval(config[\"model\"][\"first_decay_steps\"])\n",
    "use_adamw = eval(config[\"model\"][\"use_adamw\"])\n",
    "use_mixup = eval(config[\"model\"][\"use_mixup\"])\n",
    "use_swa = eval(config[\"model\"][\"use_swa\"])\n",
    "label_smoothing = eval(config[\"model\"][\"label_smoothing\"])\n",
    "n_train_splits = eval(config[\"model\"][\"n_train_splits\"])\n",
    "n_test_splits = eval(config[\"model\"][\"n_test_splits\"])\n",
    "\n",
    "label_names = {\n",
    "    \"c0\": \"safe driving\",\n",
    "    \"c1\": \"texting - right\",\n",
    "    \"c2\": \"talking on the phone - right\",\n",
    "    \"c3\": \"texting - left\",\n",
    "    \"c4\": \"talking on the phone - left\",\n",
    "    \"c5\": \"operating the radio\",\n",
    "    \"c6\": \"drinking\",\n",
    "    \"c7\": \"reaching behind\",\n",
    "    \"c8\": \"hair and makeup\",\n",
    "    \"c9\": \"talking to passenger\",\n",
    "}\n",
    "\n",
    "labels = list(label_names.keys())\n",
    "num_classes = len(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=project_name, reinit=False)\n",
    "\n",
    "if len(run_name) > 0:\n",
    "    wandb.run.name = run_name\n",
    "    wandb.run.save()\n",
    "    \n",
    "wandb.config = {\n",
    "    \"validate_by_driver\": validate_by_driver,\n",
    "    \"n_tfrec_chunks\": n_tfrec_chunks,\n",
    "    \"model_url\": model_url,\n",
    "    \"fc_size\": fc_size,\n",
    "    \"img_size\": img_size,\n",
    "    \"n_epochs\": n_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"initial_learning_rate\": initial_learning_rate,\n",
    "    \"first_decay_steps\": first_decay_steps,\n",
    "    \"use_adamw\": use_adamw,\n",
    "    \"use_mixup\": use_mixup,\n",
    "    \"use_swa\": use_swa,\n",
    "    \"label_smoothing\": label_smoothing,\n",
    "    \"n_train_splits\": n_train_splits,\n",
    "    \"n_test_splits\": n_test_splits,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab94873",
   "metadata": {},
   "source": [
    "## The Input Data Pipeline Configuration with *Dataset* API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c76bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_data_path = os.path.join(raw_data_path, \"imgs\", \"train\")\n",
    "test_raw_data_path = os.path.join(raw_data_path, \"imgs\", \"test\")\n",
    "\n",
    "n_train_examples = len(\n",
    "    tf.io.gfile.glob(os.path.join(train_raw_data_path, os.path.join(\"*\", \"*.jpg\")))\n",
    ")\n",
    "test_ids = [\n",
    "    os.path.basename(img_path)\n",
    "    for img_path in np.sort(tf.io.gfile.glob(os.path.join(test_raw_data_path, \"*.jpg\")))\n",
    "]\n",
    "n_test_examples = len(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d54e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrec_paths = tf.io.gfile.glob(\n",
    "    os.path.join(raw_data_path, \"tfrec\", \"train\", \"*.tfrec\")\n",
    ")\n",
    "test_tfrec_paths = np.sort(\n",
    "    tf.io.gfile.glob(os.path.join(raw_data_path, \"tfrec\", \"test\", \"*.tfrec\"))\n",
    ").tolist()\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        ShiftScaleRotate(\n",
    "            rotate_limit=(-20, 20),\n",
    "            scale_limit=(0.0, 0.2),\n",
    "            shift_limit_x=(-0.0625, 0.0625),\n",
    "            shift_limit_y=(-0.046875, 0.046875),\n",
    "            p=1.0,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "valid_transforms = train_transforms\n",
    "\n",
    "if n_test_splits > 1:\n",
    "    test_transforms = train_transforms\n",
    "else:\n",
    "    def identity_func(image=None):\n",
    "        return {\"image\": image}\n",
    "    \n",
    "    test_transforms = identity_func\n",
    "\n",
    "n_samples = 20\n",
    "sampled_dataset = load_tfrecord_dataset(\n",
    "    [train_tfrec_paths[0]],\n",
    "    (480, 640),\n",
    "    train_transforms,\n",
    "    n_samples,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "view_image(sampled_dataset, n_samples, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b56c2",
   "metadata": {},
   "source": [
    "## Building and Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612bb79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(verbose=False):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            hub.KerasLayer(\n",
    "                model_url, trainable=False, input_shape=(img_size, img_size, 3)\n",
    "            ),\n",
    "            tf.keras.layers.Dense(fc_size, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(fc_size, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "\n",
    "    # For tensorflow 2.5 or later, use tf.keras.optimizers.schedules.CosineDecayRestarts.\n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecayRestarts(\n",
    "        initial_learning_rate, first_decay_steps\n",
    "    )\n",
    "    optimizer = (\n",
    "        tfa.optimizers.AdamW(lr_decayed_fn)\n",
    "        if use_adamw\n",
    "        else tfa.optimizers.RectifiedAdam(lr_decayed_fn)\n",
    "    )\n",
    "    if use_swa:\n",
    "        optimizer = tfa.optimizers.SWA(optimizer, start_averaging=0, average_period=10)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
    "        metrics=[\"acc\"],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5532a744",
   "metadata": {},
   "source": [
    "## Model Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26296b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = n_train_splits if n_train_splits > 1 else n_train_splits + 1\n",
    "train_steps_per_epoch = round(\n",
    "    n_train_examples * (n - 1) / n / batch_size\n",
    ")\n",
    "valid_steps_per_epoch = round(n_train_examples / n / batch_size)\n",
    "\n",
    "test_dataset = load_tfrecord_dataset(\n",
    "    test_tfrec_paths,\n",
    "    img_size,\n",
    "    test_transforms,\n",
    "    1,\n",
    "    shuffle=False,\n",
    "    num_classes=num_classes,\n",
    "    is_prediction=True,\n",
    ")\n",
    "\n",
    "if n_train_splits > 1:\n",
    "    kf = KFold(n_splits=n_train_splits, shuffle=True, random_state=42)\n",
    "    split = kf.split(range(len(train_tfrec_paths)))\n",
    "else:\n",
    "    rs = ShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "    split = rs.split(range(len(train_tfrec_paths)))\n",
    "\n",
    "model_path = os.path.join(\"..\", \"models\")\n",
    "prediction_path = os.path.join(\"..\", \"predictions\")\n",
    "\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "os.makedirs(prediction_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f97e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_index, valid_index) in enumerate(split):\n",
    "    with get_elapsed_time():\n",
    "        one_train_dataset = load_tfrecord_dataset(\n",
    "            np.array(train_tfrec_paths)[train_index],\n",
    "            img_size,\n",
    "            train_transforms,\n",
    "            batch_size,\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "\n",
    "        if use_mixup:\n",
    "            other_train_dataset = load_tfrecord_dataset(\n",
    "                np.array(train_tfrec_paths)[train_index],\n",
    "                img_size,\n",
    "                train_transforms,\n",
    "                batch_size,\n",
    "                num_classes=num_classes,\n",
    "            )\n",
    "            zipped = tf.data.Dataset.zip((one_train_dataset, other_train_dataset))\n",
    "            train_dataset = zipped.map(\n",
    "                lambda x, y: mixup_dataset(x, y, alpha=0.2),\n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "            )\n",
    "        else:\n",
    "            train_dataset = one_train_dataset\n",
    "\n",
    "        valid_dataset = load_tfrecord_dataset(\n",
    "            np.array(train_tfrec_paths)[valid_index],\n",
    "            img_size,\n",
    "            valid_transforms,\n",
    "            batch_size,\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "        \n",
    "        if n_train_splits > 1:\n",
    "            suffix = \"_\" + str(i).zfill(2)\n",
    "            print(\n",
    "                f\"Model training started on the {i + 1} of {n_train_splits} CV sets.\"\n",
    "            )\n",
    "        else:\n",
    "            suffix = \"\"\n",
    "            \n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                os.path.join(model_path, f\"model{suffix}.h5\"),\n",
    "                monitor=\"val_loss\",\n",
    "            ),\n",
    "            WandbCallback(),\n",
    "        ]\n",
    "\n",
    "        model = get_model()\n",
    "        model.fit(\n",
    "            train_dataset,\n",
    "            epochs=n_epochs,\n",
    "            steps_per_epoch=train_steps_per_epoch,\n",
    "            validation_data=valid_dataset,\n",
    "            validation_steps=valid_steps_per_epoch,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "        for j in range(n_test_splits):\n",
    "            if n_test_splits > 1:\n",
    "                print(\n",
    "                    f\"Model prediction started with {j + 1} out of {n_test_splits} TTAs.\"\n",
    "                )\n",
    "\n",
    "            prediction = model.predict(\n",
    "                test_dataset,\n",
    "                steps=n_test_examples,\n",
    "                use_multiprocessing=True,\n",
    "            )\n",
    "\n",
    "            if j == 0:\n",
    "                predictions = prediction\n",
    "            else:\n",
    "                predictions += prediction\n",
    "\n",
    "        predictions /= n_test_splits\n",
    "        predictions = pd.DataFrame(predictions, columns=labels, index=test_ids)\n",
    "        predictions.index.name = \"img\"\n",
    "        predictions.to_csv(os.path.join(prediction_path, f\"pred{suffix}.csv\"))\n",
    "\n",
    "        if i == 0:\n",
    "            result = predictions\n",
    "        else:\n",
    "            result += predictions\n",
    "\n",
    "result /= n_train_splits\n",
    "result.to_csv(os.path.join(prediction_path, \"submission.csv\"))\n",
    "\n",
    "print(\"The model training and prediction tasks have been successfully completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowerest\n",
    "sampled_ids = np.random.choice(test_ids, n_samples)\n",
    "columns = [\"image_id\", \"image\", \"prediction\"]\n",
    "\n",
    "data = []\n",
    "for each_id, predicted in zip(\n",
    "    sampled_ids, predictions.loc[sampled_ids].values.argmax(axis=1)\n",
    "):\n",
    "    data.append(\n",
    "        [\n",
    "            each_id,\n",
    "            wandb.Image(os.path.join(test_raw_data_path, each_id)),\n",
    "            label_names[f\"c{predicted}\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "pred_img_table = wandb.Table(data=data, columns=columns)\n",
    "wandb.log({\"pred_img_table\": pred_img_table})\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo Label, GladCam, TF Serving\n",
    "# SageMaker BiT, BYOS, Debugger"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.p3.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
