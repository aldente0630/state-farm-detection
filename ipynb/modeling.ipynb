{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070991c-0d3f-4bc5-b3c4-f88759abe5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wanddb\n",
    "# Mixup + Cutmix\n",
    "# Pseudo Label, TF Serving, gpustat\n",
    "# SageMaker BiT, GladCam, BYOS, Debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c32c1-f82f-49e7-819e-847ecf9dd2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e05125-2b7f-4587-a4d9-69fcc6320a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import KFold\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    HorizontalFlip,\n",
    "    HueSaturationValue,\n",
    "    ImageCompression,\n",
    "    RandomBrightnessContrast,\n",
    "    Rotate,\n",
    ")\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils.common import get_cpu_count\n",
    "from utils.data_utils import (\n",
    "    _parse_function,\n",
    "    dump_tfrecord,\n",
    "    load_tfrecord_dataset,\n",
    ")\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef1db9-6388-4621-beb1-6fdeb7d1d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_image(dataset, n_samples, label_names=None):\n",
    "    images, labels = next(iter(dataset))\n",
    "    images = images.numpy()\n",
    "    labels = labels.numpy()\n",
    "\n",
    "    n_cols = 5\n",
    "    n_rows = n_samples // n_cols if n_samples % n_cols == 0 else n_samples // n_cols + 1\n",
    "    fig = plt.figure(figsize=(n_cols * 4, n_rows * 3))\n",
    "    for i in range(n_samples):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(images[i])\n",
    "        label = (\n",
    "            labels[i].decode(\"utf-8\")\n",
    "            if label_names is None\n",
    "            else label_names[\"c\" + str(np.argmax(labels[i]))]\n",
    "        )\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "        ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09a180-fb45-4bfa-bdd2-509c264d1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "_ = config.read(os.path.join(\"..\", \"conf\", \"config.ini\"))\n",
    "\n",
    "raw_data_path = config[\"project\"][\"raw_data_path\"]\n",
    "validate_by_driver = eval(config[\"project\"][\"validate_by_driver\"])\n",
    "n_tfrec_chunks = eval(config[\"project\"][\"n_tfrec_chunks\"])\n",
    "\n",
    "model_url = config[\"model\"][\"model_url\"]\n",
    "trainable = eval(config[\"model\"][\"trainable\"])\n",
    "img_size = eval(config[\"model\"][\"img_size\"])\n",
    "n_epochs = eval(config[\"model\"][\"n_epochs\"])\n",
    "batch_size = eval(config[\"model\"][\"batch_size\"])\n",
    "initial_learning_rate = eval(config[\"model\"][\"initial_learning_rate\"])\n",
    "first_decay_steps = eval(config[\"model\"][\"first_decay_steps\"])\n",
    "use_adamw = eval(config[\"model\"][\"use_adamw\"])\n",
    "use_swa = eval(config[\"model\"][\"use_swa\"])\n",
    "label_smoothing = eval(config[\"model\"][\"label_smoothing\"])\n",
    "n_train_splits = eval(config[\"model\"][\"n_train_splits\"])\n",
    "n_test_splits = eval(config[\"model\"][\"n_test_splits\"])\n",
    "\n",
    "label_names = {\n",
    "    \"c0\": \"safe driving\",\n",
    "    \"c1\": \"texting - right\",\n",
    "    \"c2\": \"talking on the phone - right\",\n",
    "    \"c3\": \"texting - left\",\n",
    "    \"c4\": \"talking on the phone - left\",\n",
    "    \"c5\": \"operating the radio\",\n",
    "    \"c6\": \"drinking\",\n",
    "    \"c7\": \"reaching behind\",\n",
    "    \"c8\": \"hair and makeup\",\n",
    "    \"c9\": \"talking to passenger\",\n",
    "}\n",
    "\n",
    "labels = list(label_names.keys())\n",
    "num_classes = len(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48800a44-12aa-4b15-9d21-b55e59819da0",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### *TFRecord* Files Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314ba51-b516-4ab6-9221-e17859a4ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_data_path = os.path.join(raw_data_path, \"imgs\", \"train\")\n",
    "\n",
    "train_examples = []\n",
    "for label in os.listdir(train_raw_data_path):\n",
    "    img_paths = tf.io.gfile.glob(os.path.join(train_raw_data_path, label, \"*.jpg\"))\n",
    "    for img_path in img_paths:\n",
    "        file_name = os.path.join(label, os.path.basename(img_path))\n",
    "        train_examples.append((img_path, label, file_name))\n",
    "\n",
    "n_train_examples = len(train_examples)\n",
    "\n",
    "train_proc_data_path = os.path.join(raw_data_path, \"tfrec\", \"train\")\n",
    "if os.path.exists(train_proc_data_path):\n",
    "    shutil.rmtree(train_proc_data_path)\n",
    "\n",
    "if validate_by_driver:\n",
    "    driver_imgs_list = pd.read_csv(os.path.join(raw_data_path, \"driver_imgs_list.csv\"))\n",
    "    driver_imgs_list[\"img_path\"] = (\n",
    "        driver_imgs_list[\"classname\"] + os.path.sep + driver_imgs_list[\"img\"]\n",
    "    )\n",
    "    img_paths_by_driver = (\n",
    "        driver_imgs_list.groupby(\"subject\")[\"img_path\"].apply(list).to_dict()\n",
    "    )\n",
    "\n",
    "    for key, value in img_paths_by_driver.items():\n",
    "        train_examples_by_driver = [\n",
    "            example for example in train_examples if example[2] in value\n",
    "        ]\n",
    "        np.random.shuffle(train_examples_by_driver)\n",
    "        dump_tfrecord(\n",
    "            train_examples_by_driver,\n",
    "            os.path.join(train_proc_data_path, f\"{key}.tfrec\"),\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "\n",
    "else:\n",
    "    np.random.shuffle(train_examples)\n",
    "    for i in range(n_tfrec_chunks):\n",
    "        if i == 0:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = end\n",
    "        if i == n_tfrec_chunks - 1:\n",
    "            end = n_train_examples\n",
    "        else:\n",
    "            end = (i + 1) * (n_train_examples // n_tfrec_chunks)\n",
    "\n",
    "        dump_tfrecord(\n",
    "            train_examples[start:end],\n",
    "            os.path.join(train_proc_data_path, f\"{str(i).zfill(2)}.tfrec\"),\n",
    "            num_classes=num_classes, \n",
    "            is_prediction=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15659611-cd2b-4a26-b2dd-42cdae8f18cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_data_path = os.path.join(raw_data_path, \"imgs\", \"test\")\n",
    "\n",
    "test_examples = []\n",
    "img_paths = np.sort(\n",
    "    tf.io.gfile.glob(os.path.join(test_raw_data_path, \"*.jpg\"))\n",
    ").tolist()\n",
    "for img_path in img_paths:\n",
    "    file_name = os.path.basename(img_path)\n",
    "    test_examples.append((img_path, file_name))\n",
    "\n",
    "n_test_examples = len(test_examples)\n",
    "\n",
    "test_proc_data_path = os.path.join(raw_data_path, \"tfrec\", \"test\")\n",
    "if os.path.exists(test_proc_data_path):\n",
    "    shutil.rmtree(test_proc_data_path)\n",
    "\n",
    "for i in range(n_tfrec_chunks):\n",
    "    if i == 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = end\n",
    "    if i == n_tfrec_chunks - 1:\n",
    "        end = n_test_examples\n",
    "    else:\n",
    "        end = (i + 1) * (n_test_examples // n_tfrec_chunks)\n",
    "\n",
    "    dump_tfrecord(\n",
    "        test_examples[start:end],\n",
    "        os.path.join(test_proc_data_path, f\"{str(i).zfill(2)}.tfrec\"),\n",
    "        is_prediction=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fc3ab0-a0d1-4272-8305-5d37044d94d7",
   "metadata": {},
   "source": [
    "### The Input Data Pipeline Configuration with *Dataset* API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f95b29-ff51-4759-a9c0-c757d48bd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrec_paths = tf.io.gfile.glob(\n",
    "    os.path.join(raw_data_path, \"tfrec\", \"train\", \"*.tfrec\")\n",
    ")\n",
    "test_tfrec_paths = tf.io.gfile.glob(\n",
    "    os.path.join(raw_data_path, \"tfrec\", \"test\", \"*.tfrec\")\n",
    ")\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        Rotate(limit=20),\n",
    "        RandomBrightnessContrast(brightness_limit=0.1),\n",
    "        ImageCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "        HueSaturationValue(\n",
    "            hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5\n",
    "        ),\n",
    "        RandomBrightnessContrast(contrast_limit=0.2, p=0.5),\n",
    "        HorizontalFlip(),\n",
    "    ]\n",
    ")\n",
    "valid_transforms = train_transforms\n",
    "test_transforms = train_transforms\n",
    "\n",
    "n_samples = 20\n",
    "sampled_dataset = load_tfrecord_dataset(\n",
    "    [train_tfrec_paths[0]],\n",
    "    (480, 640),\n",
    "    train_transforms,\n",
    "    n_samples,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "view_image(sampled_dataset, n_samples, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed817f19-34cb-417b-8b31-ec76034586cf",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9ca9af-5ed6-415c-947e-790393184290",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        hub.KerasLayer(\n",
    "            model_url, trainable=trainable, input_shape=(img_size, img_size, 3)\n",
    "        ),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "print(model.summary())\n",
    "\n",
    "# For tensorflow 2.5 or later, use tf.keras.optimizers.schedules.CosineDecayRestarts.\n",
    "lr_decayed_fn = tf.keras.experimental.CosineDecayRestarts(\n",
    "    initial_learning_rate, first_decay_steps\n",
    ")\n",
    "optimizer = (\n",
    "    tfa.optimizers.AdamW(lr_decayed_fn)\n",
    "    if use_adamw\n",
    "    else tfa.optimizers.RectifiedAdam(lr_decayed_fn)\n",
    ")\n",
    "if use_swa:\n",
    "    optimizer = tfa.optimizers.SWA(optimizer, start_averaging=0, average_period=10)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
    "    metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e8bd6-67ee-4b60-831c-130d5962cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps_per_epoch = round(\n",
    "    n_train_examples * (n_train_splits - 1) / n_train_splits / batch_size\n",
    ")\n",
    "valid_steps_per_epoch = round(n_train_examples / n_train_splits / batch_size)\n",
    "\n",
    "kf = KFold(n_splits=n_train_splits, shuffle=True, random_state=42)\n",
    "\n",
    "model_path = os.path.join(\"..\", \"models\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "test_dataset = load_tfrecord_dataset(\n",
    "    test_tfrec_paths,\n",
    "    img_size,\n",
    "    test_transforms,\n",
    "    1,\n",
    "    shuffle=False,\n",
    "    num_classes=num_classes,\n",
    "    is_prediction=True,\n",
    ")\n",
    "workers = get_cpu_count(0.9)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(range(len(train_tfrec_paths)))):\n",
    "    train_dataset = load_tfrecord_dataset(\n",
    "        np.array(train_tfrec_paths)[train_index],\n",
    "        img_size,\n",
    "        train_transforms,\n",
    "        batch_size,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    valid_dataset = load_tfrecord_dataset(\n",
    "        np.array(train_tfrec_paths)[valid_index],\n",
    "        img_size,\n",
    "        valid_transforms,\n",
    "        batch_size,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(model_path, f\"{str(i).zfill(2)}.h5\"), monitor=\"val_loss\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=n_epochs,\n",
    "        steps_per_epoch=train_steps_per_epoch,\n",
    "        validation_data=valid_dataset,\n",
    "        validation_steps=valid_steps_per_epoch,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    for j in range(n_test_splits):\n",
    "        prediction = model.predict(\n",
    "            test_dataset,\n",
    "            steps=n_test_examples,\n",
    "            workers=workers,\n",
    "            use_multiprocessing=True,\n",
    "        )\n",
    "\n",
    "        if j == 0:\n",
    "            result = pd.DataFrame(prediction, columns=labels)\n",
    "        else:\n",
    "            result += pd.DataFrame(prediction, columns=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
